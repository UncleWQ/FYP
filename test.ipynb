{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "def load_data(root):\n",
    "    path_list = [path for path in os.listdir(root)]\n",
    "    data = []\n",
    "    label = []\n",
    "    for data_path in path_list:\n",
    "        data_org = np.load(os.path.join(root, data_path))\n",
    "        if data_path[1] == '3':  # 1: hard fall 2: soft fall 3: non-fall\n",
    "            lb = 0\n",
    "        else:\n",
    "            lb = 1\n",
    "        data.append(data_org)\n",
    "        label.append(lb)\n",
    "    return np.array(data),np.array(label)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    os.chdir('/tmp/FYP/12_DTweighted')\n",
    "\n",
    "    ROOT = '/tmp/FYP/12_DTweighted/'\n",
    "    train_path = ROOT + 'train/'\n",
    "    test_path = ROOT + 'test/'\n",
    "    trainX,trainy = load_data(train_path)\n",
    "    testX,testy = load_data(test_path)\n",
    "\n",
    "    print(load_data(train_path))\n",
    "    print(load_data(test_path))\n",
    "    \n",
    "    print(\"data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析数据维度：二维数组（128,64），8192个数据点\n",
    "\n",
    "\n",
    "数据文件命名:\n",
    "\n",
    "\n",
    "1.data_path[1] 代表hard, soft, non-fall\n",
    "\n",
    "\n",
    "2.data_path[2] 和 data_path[3]两位代表28个属性\n",
    "\n",
    "\n",
    "3.数据集里面Hard:1-9, soft:10-14, non-fall:15-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "data_example = np.load('train_new/4101010001210420_1_061_DTweighted-01.npy')\n",
    "\n",
    "out_file_path = 'data.txt'\n",
    "with open(out_file_path,'w') as f:\n",
    "    f.write(np.array2string(data_example, threshold=np.inf))\n",
    "\n",
    "print(\"Printed!\")\n",
    "print(\"Array Shape:\",data_example.shape)\n",
    "print(\"Data Shape:\",data_example.dtype)\n",
    "\n",
    "with open(out_file_path,'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取一个示例数据，做出Doppler-Time map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载.npy文件\n",
    "data_train_example = np.load('train_new/4101010001210420_1_061_DTweighted-01.npy')  # 使用实际路径替换\n",
    "\n",
    "# 设置时间轴，假设每十个数据点代表1秒，总共有64个数据点\n",
    "time_ticks = np.linspace(0, 6.4, 64)  # 64个数据点，最大值为6.4秒\n",
    "\n",
    "# 设置频率轴，从-640 Hz到+640 Hz，总共128个数据点\n",
    "frequency_ticks = np.linspace(-640, 640, 128)\n",
    "\n",
    "# 创建图像并设置大小\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# 绘制Doppler-time map\n",
    "plt.imshow(data_train_example, aspect='auto', cmap='viridis', interpolation='none',\n",
    "           extent=[time_ticks.min(), time_ticks.max(), frequency_ticks.min(), frequency_ticks.max()])\n",
    "\n",
    "# 设置颜色条和标题\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.title('Doppler-Time Map')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "\n",
    "# 调整刻度密度\n",
    "plt.xticks(np.arange(0, time_ticks.max(), 0.5))\n",
    "plt.yticks(np.arange(frequency_ticks.min(), frequency_ticks.max()+1, 128))\n",
    "\n",
    "# 保存图像，使用较高的dpi值\n",
    "plt.savefig('doppler_time_map_high_res.png', dpi=300)  # 使用高分辨率保存\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验目的：对（128,64）数据进行数据增强\n",
    "\n",
    "实验方法：Stable Diffusion\n",
    "\n",
    "之后跑分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Diffusion架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from torch.nn.functional import relu\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 检查CUDA是否可用，选择正确的设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义U-Net模型\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.time_embedding = nn.Linear(1, 16)\n",
    "        self.down1 = nn.Sequential(nn.Conv2d(1 + 16, 64, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(64))\n",
    "        self.down2 = nn.Sequential(nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(), nn.BatchNorm2d(128))\n",
    "        self.down3 = nn.Sequential(nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.ReLU(), nn.BatchNorm2d(256))\n",
    "        self.up2 = nn.Sequential(nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), nn.ReLU(), nn.BatchNorm2d(128))\n",
    "        self.up3 = nn.Sequential(nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), nn.ReLU(), nn.BatchNorm2d(64))\n",
    "        self.final = nn.Conv2d(64, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = relu(self.time_embedding(t))\n",
    "        t_emb = t_emb.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, x.shape[2], x.shape[3])\n",
    "        x = torch.cat([x, t_emb], dim=1)\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x = self.up2(x3)\n",
    "        x = self.up3(x + x2)\n",
    "        x = self.final(x + x1)\n",
    "        return x\n",
    "\n",
    "# 自定义数据集\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith('.npy')]\n",
    "        if not self.files:\n",
    "            raise RuntimeError(\"No data files found in specified directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = np.load(self.files[idx])\n",
    "        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        if img.max() == img.min():  # Check for constant image\n",
    "            img = torch.zeros_like(img)  # Use a zero tensor if the image is constant\n",
    "        else:\n",
    "            img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0, 1]\n",
    "        return img, os.path.basename(self.files[idx])\n",
    "\n",
    "# 扩散和逆扩散步骤\n",
    "# 扩散步骤\n",
    "def diffusion_step(x, beta):\n",
    "    noise = torch.randn_like(x).to(device)\n",
    "    return torch.sqrt(torch.clamp(1 - beta, min=1e-8)) * x + torch.sqrt(torch.clamp(beta, min=1e-8)) * noise\n",
    "\n",
    "def reverse_diffusion_step(x_t, t, model, beta):\n",
    "    pred_noise = model(x_t, t)\n",
    "    return (x_t - torch.sqrt(beta) * pred_noise) / torch.sqrt(1 - beta)\n",
    "\n",
    "# 训练函数\n",
    "def train(model, data_loader, epochs=10, beta_schedule=np.linspace(0.01, 0.1, 1000)):\n",
    "    model.train()\n",
    "    name = torch.cuda.get_device_name()\n",
    "    print('Using device '+ name + ' to train the model.')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        epoch_loss = 0\n",
    "        for batch, _ in data_loader:\n",
    "            x = batch.to(device)\n",
    "            t = torch.rand(x.size(0), 1).to(device)\n",
    "            beta = torch.tensor(beta_schedule[np.random.randint(0, len(beta_schedule))], device=device)\n",
    "            x_t = diffusion_step(x, beta)\n",
    "            pred_noise = model(x_t, t)\n",
    "            true_noise = x - diffusion_step(x, beta)\n",
    "            loss = loss_fn(pred_noise, true_noise)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(data_loader)\n",
    "        losses.append(avg_loss)\n",
    "        elapsed = (time.time()-start) / 60\n",
    "        print('Training epoch={} \\t cost_time={:.3f} min \\t loss={:.6f} '.format(epoch+1, elapsed, avg_loss))\n",
    "    return losses\n",
    "\n",
    "# 采样函数和保存\n",
    "def generate_and_save_samples(model, data_loader, output_dir='train_new', beta_schedule=np.linspace(0.01, 0.1, 1000)):\n",
    "    model.eval()\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    timesteps = 1000\n",
    "    with torch.no_grad():\n",
    "        for batch, filenames in data_loader:\n",
    "            for x, filename in zip(batch, filenames):\n",
    "                x = x.unsqueeze(0).to(device)\n",
    "                x_t = torch.randn_like(x)\n",
    "                for t in reversed(range(timesteps)):\n",
    "                    time_step = torch.tensor([[t / timesteps]], device=device)\n",
    "                    beta = torch.tensor(beta_schedule[t], device=device)\n",
    "                    x_t = reverse_diffusion_step(x_t, time_step, model, beta)\n",
    "                sample_np = x_t.cpu().numpy().squeeze()\n",
    "                variant_filename = os.path.basename(filename).replace('.npy', '-01.npy')\n",
    "                np.save(os.path.join(output_dir, variant_filename), sample_np.astype(np.float64))\n",
    "\n",
    "# 实例化模型和数据加载\n",
    "model = UNet().to(device)\n",
    "dataset = ImageDataset('/tmp/FYP_Projects/12_DTweighted/train')\n",
    "dataloader = DataLoader(dataset, batch_size=50, shuffle=True,num_workers=6)\n",
    "\n",
    "print(\"Dataset loaded!\")\n",
    "# 训练模型\n",
    "losses = train(model, dataloader)\n",
    "torch.save(model.state_dict(),'/tmp/FYP_Projects/diffusion_model.pt')\n",
    "print(\"Model saved!\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # 生成样本并保存\n",
    "# generate_and_save_samples(model, dataloader)\n",
    "# print(\"Task finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LKCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LKCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(7, 1), stride=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(4, 1), stride=(4, 1))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1, 9), stride=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 30 * 14, 1024)  # 此处的乘数需要根据实际尺寸计算得到\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # 动态计算展平后的尺寸\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = LKCNN()\n",
    "print(model)\n",
    "\n",
    "# 假设有一个随机数据批次和标签\n",
    "inputs = torch.randn(1, 1, 128, 64)  # batch_size=1, channels=1, height=128, width=64\n",
    "labels = torch.randint(0, 3, (1,))  # batch_size=1, classes=10\n",
    "\n",
    "# 前向传播\n",
    "outputs = model(inputs)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss = loss_function(outputs, labels)\n",
    "\n",
    "print(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65696248c9b71dd3c48f2dd2e5b2d3607d9aed1eef8791c3764510e063b61a82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
